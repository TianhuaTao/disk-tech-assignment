# strata

Strata文件系统要解决的问题是：在大规模数据上进行大量的随机小修改，同时要求具有较低延迟、高并发和高可靠性。

Strata文件系统思路：利用NVM、SSD和HDD的不同特点，在三者整合起来的硬件上建立文字系统。简单来说，用户层的暂存log存放在user space中（对应硬件NVM），需要长时间存放的存储放在kernel中（对应硬件为SSD和HDD）

和现有文件系统相比，进行的贡献：一个同时使用了NVM、SSD和HDD的文件系统。

Strata文件系统定义了digest操作：read file system data and metadata把数据从上一层写到下一层。digest的过程会对数据进行处理，并不是简单复制：

- 对于被重写过的数据进行合并
- 合并后的数据，用适合查找的数据结构对其进行重新组织
- 对数据进行分批处理，以适应SSD和HDD的硬件特性

- 最上层的digest是：将数据从每个进程私有的空间转移到系统中进程共享的数据空间，且两者都存放在NVM当中。进行的操作是：合并多次连续的写操作，并将数据修改后的结果形成一个更大的块（block）。这个过程和LSM树进行的处理很像，但是digest做的处理比LSM树更多一些。和LSMTree相比：
  - 降低了数据重写延迟
  - 在特定场景下，减少数据的重复复制



## 背景

硬件发展多样性：DRAM,NVM,SSD,HDD等不同存储介质，这些介质在花费、延迟和带宽方面各有所长也各有所短。应对较大规模的数据，通常会需要比较大规模的block size，但与此同时，人们在应用上产生了新的需求。很多应用场景要求，保证崩溃一致性的同时，更多的数据读写是随机读写，且每次数据规模较小。

因此随之而来的就是写扩大问题（write amplification:在SSD和HDD中，一个字节的写入需要更新整个block，而文件系统在运行过程中，经常需要对数据进行更新）。对此NVM的解决思路是：利用NVM作为“缓冲”，将对随机小数据的读写和写入先存进NVM中，对数据进行合并与整理后再写入SSD和HDD，这样写入SSD和HDD中的块都是整个的block chunk。

现有的替代方案存在诸多的不足之处。现有的文件系统，例如NOVA，PMFS，F2FS，都是针对单一存储介质进行的设计。同时，现有文件系统也均未解决写扩大问题。此外，Strata还能实现对应用提供一个文件系统，而不是裸硬盘。



---

## Strata如何设计？

结合不同硬件特点，Strata文件系统可以提供以下优势：

- 快速写入
- 提高异步写入的速度
- 解决了写扩大问题
- 高并发
- 统一的接口

它的基本结构很像LSM树。运行时log写入NVM，延迟小且实现可持久化。但是log适合写入，却不适合数据读取。因此这些数据会被定期重新组织为更加方便执行读操作的数据结构，这个过程被称为digest。当每次digest发生后，会对log进行垃圾回收。

Strata文件系统分为两层：LibFS和kernelFS。LibFS负责控制一部分的NVM，具有读写权限；它负责管理不同进程的log，并且对kernelFS管理的硬件部分具有可读权限。kernelFS负责digest，以同步、多线程并行的方式，对文件系统的元数据进行更新。当digest完成后，这块更新过的数据就可以被系统中的其他进程读取。如果在digest过程中突然发生crash，kernelFS就可以将没有完成digest的log继续执行digest操作；在一次完整的digest过后，log会被垃圾回收，方便这块空间再次被分配给其他进程。以上步骤都是在NVM内完成的。

Strata通过这个设计，能够使数据被写入SSD和HDD时，以大块顺序的方式写入。

### 元数据结构

Strata文件系统的元数据存储在DRAM中，和其他文件系统的结构很相似。**Superblock**存储在NVM中，它记录的是每一个存储层的layout，以及每一个进程数据的log的存放地址，它由KernelFS进行维护。**Inode**用于存储文件的元数据：访问权限，作者和创建时间等；一个inode有多个root，对于每一层存储介质(NVM,SSD,HDD)都会创建一个root；扩展树的节点直接指向一个文件的数据块。**Directory**和ext4文件系统很相似：保存了一系列的文件名和对应的inode number；在文件要进行读取的时候，LibFS先寻找每一个inode对应的pointer；digest之后，指针也会随之失效。为了保护inode，不同访问权限的inode放在不同的页里，或者在不同命名空间之下。

### LibFS

LibFS的功能是：管理每一个进程的log。对于每一个进程，LibFS会为其分配一个私有的空间，用于存储它产生的log。当log 的大小达到阈值时，有KernelFS负责进行digest。

**快速异步持久化。**对于传统文件系统，持久化实现是要先将数据写入DRAM中，再等待DRAM将文件内容写入操作系统的页面。但是对于LibFS，当一个应用程序产生写请求时，会绕过OS而直接写入文件系统。由于从DRAM复制到OS和NVM读写产生的延迟相当，因此省去DRAM复制的时间后会明显降低延迟。

**崩溃一致性**。每一个进程的log都写在NVM上，崩溃一致性是由NVM的硬件特性保证的。如果想要进一步进行可持久化，digest过程会根据已经写入的log size大小自动进行持久化。如果一次系统调用就涉及到非常大的数据，就将其拆分为多个transaction。每一个log表项包括：header,relevant updates to file (meta-)data和commit record.每一个transaction包括很多log entry。

**digest后的垃圾回收**。当digest过程完成后，就可以对涉及到的log块进行垃圾回收，以便它们被分配给其他进程。垃圾回收过程为：更新log header（重置header中的valid bit，这样log的空间就可以被重新分配），并更新对应的data cache。垃圾回收是在后台进行的，不会影响应用程序的正常IO。且由于垃圾回收对log表项的操作互相独立，因此如果在垃圾回收期间系统产生崩溃，也不会产生不一致问题，在系统恢复后，继续进行回收即可。

**快速读数据**。由于NVM访问速度较快且支持字节访问，因此只有从SSD或HDD中读数据时，才会将数据缓存到DRAM中，如果从NVM中读数据是不缓存的。因此为了读到最新的数据，应该按照DRAM，NVM，SSD，HDD的顺序进行搜索。

### KernelFS

KernelFS的功能是：管理共享的数据，对于系统中所有的进程都可见，数据可以被储存在3个不同的硬件层。KernelFS会定期调用digest，将LibFS中的log转换为更加适合读的perfile extent trees，并存入KernelFS中，使其对所有进程可见。同时，这个过程是在backgroud中执行的，也不会等到log已经存满了才执行，因此便于优化存储结构

KernelFS负责**digest**。digest指的是，每一个进程启动时，都会在LibFS中为其分配一个私有的空间，用于存储这个进程对文件的修改log。同时设置一个阈值，当log size超过阈值或进程结束时，就会启动digest。但是问题在于，当为进程分配的空间写入log已经快写满时，digest的延迟就会影响到IO。针对此情况进行优化：合并临近的写操作，将紧邻的写操作作为一次写操作。这个优化可以增大平均写操作的写入文件大小，同时也减少了数据合并后的扩展树的深度。同时，在digest前会对文件log进行扫描，从最后一次digest的文件开始写入kernelFS。

KernelFS负责管理NVM、SSD和HDD中的数据存储，它会在DRAM中为NVM层和SSD层分别维护LRU栈，根据访问时间决定将哪些数据放在上层；不同硬件之间获取数据通过系统调用。数据迁移过程对应用透明，是在后台进程中进行的；而为了数据迁移过程不影响IO，通常在某一层的空间使用95%时就会开始迁移过程。数据迁移过程类似digest，在数据迁移过程完成后，也会对原先存放数据的空间进行释放。

### 进程之间数据共享

有时，不同进程是需要共享同一数据的：某一进程对数据进行修改后，另一进程要求立刻可见该修改。这时怎么做呢？Strata给出的解决方案是：在文件系统中某一特定目录，开辟一个特殊的命名空间，专门用于存放需要共享的数据。lease类似“读写权限”，同时只有一个进程可以拿到写权限。当某一个进程正在写数据时，如果另一个进程也发出写数据请求，那么正拥有写权限的进程会被一个上层调用(upcall)通知。

### 访问权限

在Strata系统中，通过命名空间来进行访问权限控制。但是由于文件系统的限制，最多有$2^{32}$个命名空间，因此对于同时打开的文件数量有限制。



---

## Implementation

对硬件的访问：SPDK;extent tree: EXT4

### 存在的限制

**LibFS和kernelFS之间沟通要用到系统调用。**由于kernelFS和LibFS运行在不同的进程，因此需要通过系统调用来进行沟通，而一次系统调用的开销是很大的。不过相信这个产生的影响不会太大，因为Strata的设计就是为了通过LibFS做可持久化的缓冲，对数据进行整合来尽可能地减少kernelFS运行次数（当然也就减少了系统调用次数）。

对于文件夹没有完全实现**进程之间的数据共享**。

对于**需要进行memory map的文件，写扩大的问题难以解决。**尤其是对于大量的随机小更改，写扩大现象会更加严重。不过，对于权限为只读的mapping，或者是更改仅对此进程可见的mapping，就可以将内存映射到NVM上。但是，如果mapping之后的权限是不同进程可以共享，在某一进程之内进行的更改，其他进程要立刻见到的话，这时无法使用log方式（因为log只有在digest过程后，数据才对其他进程可见），写扩大问题也就无法解决。

**没有针对硬件进行容错。**第一个问题是，目前的文件系统没有为硬件上的错误预留出空间，不过由于它将数据存储在了不同的存储介质上，因此失去的数据可以在三者之间取最小。此外，在Strata文件系统下，一个关机的机器上如果某一个存储介质被移走，也不会产生任何警告。

---

## 相关工作

### log

以前也有使用log思路的，但是他们用于支持元数据而不是数据。

### 多层存储

以前的多层存储都是用DRAM和SSD、HDD一起的，这样的文件系统有很多。改进在于使用了NVM的方式。

---

## 一点问题

本篇文章在2018？（2019？？？）年写，但是在统计硬件数据时使用的是2015年论文数据。经过几年的发展，硬件表现应该会有所提升（尤其NVM）。





